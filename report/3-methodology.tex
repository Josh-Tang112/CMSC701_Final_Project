\section{Methodology}
We implemented our solution in C in order to take advantage of the library
\zlib~\cite{zlib}. \zlib is a C library written and maintained by Jean-loup
Gailly and Mark Adler since 1995.
The software tools we developed for our project consist of an \emph{index
building} component, which creates the sequence-aware \gzip index, and an
\emph{index reading} component, which we use to validate the utility of the
index building component by using the indices to read a compressed gzip file in
parallel and write it out to a file, which we then compare against the
uncompressed original FASTQ to verify our program's correctness. We explored two 
approaches to building the index. Subsection~\ref{sec:sync} documents the design of 
an index builder and parallel parser predicated on the idea of inserting sync points
into a \gzip file. However, given the severe limitations of this approach, we discuss
the design of an index builder and reader based on modifications to \zran in
Subsection~\ref{sec:ibuilder} and Subsection~\ref{sec:ireader}, respectively.

\subsection{Sync Point Index}
\label{sec:sync}

We first explored a simple index implementation in an attempt to avoid the complications and 
limitations of \zran. In particular, we sought to address two main issues. First, recall that 
the decoder must have access to the 32 KB window of data immediately preceding a \gzip block 
in order to resolve back references. Storing at least 32 KB per \gzip block can be expensive, 
and these windows will dominate the size of the index. Second, recall that decompression can 
only start from block boundaries. This is undesirable for a parallel FASTQ parser for multiple 
reasons: we will have limited control over the frequency of points where we can randomly access 
the \gzip file, and it will be difficult to deal with reads that span multiple \gzip blocks.

We therefore took advantage of the Z\_FULL\_FLUSH option of \zlib. When compressing data with
Z\_FULL\_FLUSH, \zlib forces the completion of a \gzip block, performing byte-alignment, adding
a four-byte marker sequence to act as a sync point, and resetting the compression state so that
no back references in the subsequent block can point before the sync point and decompression can 
start from the sync point without the 32 KB window. Building an index over a \gzip file with sync
points would allow us to eliminate the need to store 32 KB per block, and it would allow us to
start decompression from arbitrary sunc points in the file as opposed to block boundaries. 
Unfortunately, most \gzip files are not created using the Z\_FULL\_FLUSH option because it degrades
compression. In order to build an index over any given \gzip file, we must first decompress it and
recompress it to add sync points at desired locations. 

We implemented a C program to insert a sync point after recompressing a user-configurable number of 
reads (collectively known as a ``chunk'') and simultaneously build an index over the new file. Each 
index entry stores a triplet of integers representing the byte offset in the compressed stream where 
the sync point appears, the last read before the sync point, and the total number of bytes in the 
uncompressed stream that have been compressed up until this point. Technically, only the offsets of 
each sync point are strictly necessary for parsing entire chunks of reads in parallel. However, storing 
the last read in each chunk allows us to have the flexibility of retrieving specific ranges of reads within 
the \gzip file rather than entire chunks, and knowing the total number of bytes in the uncompressed stream 
that have been compressed up until a sync point simplifies memory allocation. Even with storing these two
extra integers, we can store the information for over 1,000 sync points in the same amount of space as the
32 KB windows. 

We implemented a second C program capable of reading chunks of a \gzip FASTQ file in parallel (using a
configurable number of threads) given the index over the file, a starting chunk, and a number of chunks 
to read. Each thread is responsible for an equal-sized subset of the range of chunks. A thread will
seek directly to the byte offset of the sync point corresponding to the first chunk in the range, 
decompresses the entire range of chunks, and return the result.

We note however that this approach to building indices has severe limitations, which in turn
limited our ability to effectively test it. First, the time to build the index is tied to the
time needed to recompress a \gzip file, which is slow due to the size of the uncompressed data
and the use of file I/O. Furthermore, we require extra disk space that is at least twice the
size of the original \gzip file in order to create a copy with sync points. Because sync points
add additional bytes due to byte alignment and byte markers, the new \gzip is likely to be
somewhat larger than the original. With sufficiently frequent sync points, the increase in file
size may offset the space saved by not storing 32 KB windows of data in the index.

\subsection{\ibuilder}
\label{sec:ibuilder}
Given the limitations of our sync point index approach, we built our alternative \ibuilder 
tool around the idea of access point indices from \zran. Because \zran was not designed for 
FASTQ files or genomics files in general, we have to modify \zran to add more information to 
our index. Specifically, we wish to create indices only for \gzip blocks that contain the 
start of a chunk of reads (\eg 10,000), a number that can be specified by the user. 
Therefore, we need to restrict the program to only create indices at the start of 
blocks that contains the start of a sequence read chunk.

This, however, creates a new set of problems. The start of each \gzip block
contains information needed to decompress the block. If the index is not on
block boundary, the inflate algorithm will fail to execute unless we store the
Huffman code tree at the start of each block alongside the index. We decided
against this approach and instead chose to store the index of start of the corresponding
block for each index and how many uncompressed bytes we need to skip instead. 

The reason for this is simple. If the index is at the middle of a block, we 
have to tell the inflate algorithm how many bits in the byte that the index points 
to it should skip. In other words, we need to know the valid starting point in the 
byte which standard \zlib~\cite{zlib} will not provide. That means we would have to 
write our own inflate algorithm, which is impossible due to time constraints and 
our limited knowledge about the implementation of the inflate algorithm. 

The downside of this approach is quite limited considering two facts at hand.
First, we won't create an index until both the block and the read at the end of
the block end, which means in the worst case scenario, we only have
one-read-length amount of bytes of overhead per index. Second, each read is
short in FASTQ. If we can control the number of index points, we can make the
overhead of our approach negligible.  

Our \ibuilder implementation is written in C and uses \zlib; indeed, it was
created by modifying \zran directly. Like \zran, the end of each DEFLATE block,
\ibuilder adds a new \texttt{access\_point} index entry to a list of index
points that it maintains while reading the \gzip FASTQ file. Unlike \zran, we
also parse the decoded byte buffer to count the line numbers within the FASTQ
file as we decompress it. We do this because the FASTQ format specifies that
every fourth line is the start of a new sequence. Therefore, by maintaining
state on what line number we are decoding as we inflate the compressed file, we can
track which sequence we are on by incrementing a counter every fourth line. 

In addition to the required \gzip file parameter, \ibuilder accepts an
optional \texttt{-c CHUNKSIZE} parameter that allows the user to specify the
number of sequence reads in a ``chunk.'' When the \ibuilder has read
\texttt{CHUNKSIZE} sequences (by default 10,000), it denotes which DEFLATE block
number it is currently in, and the number of bytes from the start of the current
block the sequence chunk begins at. It adds this information to another linked
list in order to output this information when the program completes. In this
manner, \ibuilder creates an index over the \gzip FASTQ file, annotating both
the start of \gzip DEFLATE blocks and the start of sequence chunks within those
blocks. 

When the \ibuilder program has finished reading the file, it outputs two index
files -- \texttt{output.idx} and \texttt{output.seq-idx}. The former contains
information allowing a reader to begin decompression at the beginning of a
DEFLATE block -- the number of bytes the block starts from the beginning of the
compressed file, what byte number that point is in the uncompressed data, and
32kB worth of data immediately preceding it in order to initialize the
decompression state. The latter contains information that allows the reader to
seek from the beginning of a DEFLATE block to the beginning of a sequence chunk
-- namely, which DEFLATE block number the sequence chunk occurs within, and the
byte offset, which allows a reader to calculate how much data exists between the
start of the block and the start of the chunk, which must be discarded.

\subsection{\ireader}
\label{sec:ireader}

Like \zran, our \ireader will skip to the desired DEFLATE block while
maintaining the necessary context for inflate algorithm. After it fetches the
block and creates the context, it will read all the bytes from this index up to
the next index, decompress them, and return the result to user. In our initial
prototype, we have \ireader write the decompressed FASTQ file to disk. While
this task is not specifically related to genomics, it allows us to make easy
benchmarking comparisons to single-threaded tools that accomplish the same task,
such as \texttt{zcat}, which simply decompresses a \gzip file. It also allows
us to easily compare our parallel, FASTQ-aware \ireader implementation's output
with the decompressed FASTQ file to validate the correctness of our
implementation.

\ireader has three mandatory arguments and one optional argument. A user must
provide the \texttt{.idx} and \texttt{.seq-idx} files produced by \ibuilder, as
well as the \gzip file that they were built over. The user may also specify the
number of threads they wish to use to read the file in parallel; the number of
threads is set to 4 by default. The allowed thread number is bounded between 1
and 16.

\begin{table*}[ht]
    \centering
    \caption{Four sources of FASTQ data were used in our study. The FASTQ files
    were \gzip compressed for our index-building and parallel reading
    experiments.}
\begin{tabular}{r|l|r|r}
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Sequence Read\\
Identifier\end{tabular}}} & \multicolumn{1}{c}{\textbf{Source}} &
    \multicolumn{1}{c}{\textbf{Sequence Reads}} &
    \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}FASTQ GZ \\ Size (MB)\end{tabular}}} \\
\hline\\
SRR3295681 (Small)& Salmonella enterica & 959,879 & 205\\
SRR2121685 (Medium) & Mus musculus & 27,928,438 & 2,078\\
SRR925811  (Large) & Homo sapiens & 53,265,409 & 3,349 \\
SRR925816 (XL) & Homo sapiens & 71,504,007 & 5,046
\end{tabular}
    \label{tab:source}
\end{table*}

\subsection{Data Sources}

We used four reference FASTQ sequence reads from the Sequence Read
Archive~\cite{SRA} for our testing and analysis. Table~\ref{tab:source} is a
tabular depiction of the data sets that we used for this project. We chose these
files because they represented a wide range of numbers of sequence reads, which
translates into a wide range of \gzip file sizes for our tool to contend with.
Indeed, the largest FASTQ \gzip file we consider is approximately 25 times as
large as the shortest.

